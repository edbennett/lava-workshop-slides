# Context

-

## Open science

The movement to make all research accessible to all levels of society.

Including, but not limited to:

- Publications
- Physical samples
- Data
- Software

-

## Why open science?

- The ideal scientific process
- Public funding $\Rightarrow$ Public results <!-- .element: class="fragment fade-in" data-fragment-index="2" -->
- Funders, journals, and institutions say so <!-- .element: class="fragment fade-in" data-fragment-index="3" -->

Researchers starting out in the field today will have significantly higher expectations placed on them for open science than was previously the case. <!-- .element: class="fragment fade-in" data-fragment-index="4" -->

We need to prepare them for this! <!-- .element: class="fragment fade-in" data-fragment-index="4" -->

-

## Reproducibility

Same data
<span class="fragment fade-in" data-fragment-index="1">$+$ same analysis</span>
<span class="fragment fade-in" data-fragment-index="2">$\rightarrow$ Same results</span>

- Related concepts: <!-- .element: class="fragment fade-in" data-fragment-index="3" -->
  - Replicability: New data $+$ same analysis $\rightarrow$ same results <!-- .element: class="fragment fade-in" data-fragment-index="4" -->
  - Robustness: Same data $+$ new analysis $\rightarrow$ same results <!-- .element: class="fragment fade-in" data-fragment-index="5" -->

-

## Why reproducibility?

If findings are not reproducible, are they science?

-

## Why _automate_ reproducibility?

It is much _easier_ than the alternative!

* Communication with words is imprecise
* Papers have limited space
* Human error is inevitable
* Computers are pretty good at doing the same thing every time

-

## State of the field (2021)

![Bar chart showing the prevalence of various aspects of reproducibility/open science in lattice](images/repro_summary.svg)

([Data: 10.5281/zenodo.6584001](https://doi.org/10.5281/zenodo.6584001))

-

## Why aren't people doing more already?

![Bar plots of "Why have you not made your data available" in most recent publication and in any publication](images/why_not_made_data_available.svg) <!-- .element width="650px"-->
![Bar plots of "Why have you not made your code available" in most recent publication and in any publication](images/why_not_made_code_available.svg) <!-- .element width="650px"-->

([Data: 10.5281/zenodo.6980070](https://doi.org/10.5281/zenodo.6980070))

-

## What would incentivise people to do more?

![Word cloud with "incentives" and "guidelines" as the largest words](images/open_science_incentives_cloud.svg) <!-- .element width="1000px"-->

---

# Proposed syllabus

-

## High-level topics

- Open science
  - Open access publications
  - Open data
  - Open workflows
  - Open software development
- Reproducibility
  - Common aspects
  - Reproducible configuration generation
  - Reproducible measurements
  - Reproducible analysis
  - Reproducible presentation

-

## Open Access Publications

- Green and Gold open access
- Creative Commons Licensing
- The arXiv
- Institutional policies and repositories
- Rights retention

-

## Open Data

- FAIR
- Metadata and schemas
- Data formats
- Zenodo
- ILDG

-

## Open software development

- Version control
- GitHub
  - Issue tracking
  - Pull requests
- Automated testing
- Agile techniques

-

## Open Workflows

- Software licensing
- GitHub&ndash;Zenodo integration
- Why not GitHub?
- Combine with data, or publish separately?
- Disposable vs reusable tools

-

## Reproducibility: Common aspects

- Types of reproducibility
  - Bitwise vs compatible numbers
- Named software
- Identified commits
- Seeded RNGs
- Stable parallelism
  - SIMD, MPI, GPUs, race conditions, ...
- Environment specification
  - Pros and Cons of containers, other ways of env. spec.

-

## Reproducible configuration generation

- Persisting RNG state
- Provenance
  - When generated
  - Where generated
  - Generated by whom
  - Including __all parameters_
- Configuration and metadata storage formats

-

## Reproducible measurements

- Provenance
  - What ensembles analysed
  - Where from
  - Including _all parameters_
- Data I/O formats

-

## Reproducible analysis

- Provenance
  - What data analysed
  - Where from
- Data I/O formats
- Code structure
  - One large tool vs many smaller ones
- Workflow and data-flow management
- Removing manual steps

-

## Reproducible presentation


- Outputting tables
- Styling plots
- Outputting single values
- Keeping data consistent
- RMarkdown and similar technologies?
- Suggest avoiding Jupyter Notebooks?
- Removing manual steps

---

# Existing material

-

## Existing material: Open access publications

- University libraries likely have material on institutional policies and procedures

-

## Existing material: Open data

- Currently a shortage of metadata standards for LFT, so little material
- ILDG may already have material on configuration sharing

-

## Existing material: Open software development

- [Version control from Software Carpentry](https://swcarpentry.github.io/git-novice)
- GitHub has their own tutorial material
- A couple of (Python-specific) workshop courses on automated testing:
  [1](https://edbennett.github.io/python-testing-ci) [2](https://carpentries-incubator.github.io/python-testing/)
- Agile techniques

Most topics also covered in
[Research Software Engineering with Python (Irving et al.)](https://merely-useful.tech/py-rse/)

Examples may benefit from being made more lattice-specific

-

## Existing material: Open workflows

- [Carpentries-style lesson contains some relevant material](https://edbennett.github.io/publishing-analysis)

-

## Existing material: Reproducibility: Common aspects

- Some aspects of containers and environment specification covered in [this workshop](https://edbennett.github.io/publishing-analysis)
- [More Carpentries Incubator material on containers](https://carpentries-incubator.github.io/docker-introduction/)
  (a more balanced take may also be useful)

-

## Existing material: Reproducible configuration generation

- Currently not aware of anything

-

## Existing material: Reproducible measurements

- Currently not aware of anything

-

## Existing material: Reproducible analysis

- Guidance on workflow managers is in preparation
- Some guidance on removing manual steps in [this workshop](https://edbennett.github.io/publishing-analysis)

-

## Existing material: Reproducible presentation

- Discussion on styling plots in
  [Research Software Engineering with Python (Irving et al.)](https://merely-useful.tech/py-rse/)

---

# Other questions

-

## Overlaps

- Statistical analysis
  - Such analysis must be reproducible
  - Such analysis should be published
- Hardware
  - Both likely to benefit from some Carpentries material

-

## Other communities/possible contributors

- ILDG (focus on gauge configuration sharing and provenance)
- PUNCH4NFDI (includes work on ILDG but also work on analysis workflows)
- NI4OS (wider project)
- The Carpentries (focus on software skills for researchers)

-

## "Beginner" vs "Advanced"

- Tricky distinction to make
- New researchers need to learn relevant topics
  - E.g. Those not generating configurationsdo not need to learn that material
- Most topics in each area are needed for all researchers looking to publish
- Potential approach:
  start prescriptive, then develop more context to enable decision making

-

## Exercises with code and data

"Would homeworks or exercises with code or data be useful 
to present new ideas in your topic?"

Yes.

-

## Prerequisites

- Difficult to engage with material before learners are producing their own data
- But don't want students to progress so far as to develop bad habits
- Integrating learning this topic with students' initial exploratory projects could solve this
- May also change as more data and workflows become open
- Assumption of already knowing a programming language
  - Software Carpentry teaches a language from scratch
